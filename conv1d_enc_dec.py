# -*- coding: utf-8 -*-
"""conv1d_enc_dec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PNRC006dN0bmdYgS4tZL44PlMdU10E_G
"""

from typing import Tuple

import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np


class ConvEncoder(nn.Module):
    def __init__(self, dropout):
        super(ConvEncoder, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(1,16,3,padding=1),
            nn.Dropout(dropout),
            nn.AvgPool1d(2,stride=2),
            nn.Conv1d(16,32,3,padding=1),
            nn.Dropout(dropout),
            nn.AvgPool1d(2,stride=2),
        )

        self.fc_mu = nn.Sequential(
            nn.Linear(32*6, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
        )
        self.fc_logvar = nn.Sequential(
            nn.Linear(32*6, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
        )


    def forward(self, x:torch.Tensor):
        """
        input: x of shape (batch_size, seq_len, channels=input_size)
        """
        assert self.fc_mu is not None, "Please call fit() before forward()"
        x = x.permute(0,2,1)
        x = self.conv_layers(x)
        x = x.flatten(1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)

        return mu, logvar



class ConvDecoder(nn.Module):
    def __init__(self, dropout):
        super(ConvDecoder, self).__init__()

        self.fc = nn.Sequential(
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, 32*6),
            nn.ReLU(),
        )

        self.deconv_layers = nn.Sequential(
            nn.ConvTranspose1d(32,16,3,stride= 2, padding=1, output_padding=1),
            nn.Dropout(dropout),
            nn.ConvTranspose1d(16,1,3,stride = 2, padding=1, output_padding=1),
            nn.Dropout(dropout),
        )

    def forward(self, z:torch.Tensor):
        """
        input: z of shape (batch_size, latent_size)
        """
        assert self.fc is not None, "Please call fit() before forward()"

        x = self.fc(z)
        x = x.view(-1,32,6)
        x = self.deconv_layers(x)
        x = x.permute(0,2,1)
        return x

class ConvSeq2Seq(nn.Module):
    def __init__(self, encoder:ConvEncoder, decoder:ConvDecoder) -> None:
        super(ConvSeq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, x:torch.Tensor):
        """
        input: x of shape (batch_size, seq_len, input_size)
        """
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        z= mu
        recon_batch = self.decoder(z)
        return recon_batch, mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def predict(self, x:torch.Tensor, target_len:int)-> np.array:
        input_tensor = pad_tensor(x.unsqueeze(0), target_len, dim=1).squeeze(0)
        print("Input shape :", input_tensor.shape)
        assert input_tensor.size(0) == 24, "len(x) + target_len should equate to 24"
        assert len(input_tensor.size()) == 2, "input_tensor should be of shape (seq_len, input_size)"

        self.eval()
        with torch.no_grad():
            recon_batch, mu, logvar = self(input_tensor.unsqueeze(0))
            return recon_batch.squeeze(0)[-target_len:].detach().cpu()


def pad_tensor(x:torch.Tensor, pad_len:int, dim:int=1):
    """
    output: padded_x of shape (batch_size, seq_len+pad_len, input_size)
    """
    assert x.dim() ==3, "x should be of shape (batch_size, seq_len, input_size)"

    pad = torch.zeros((x.shape[0], pad_len, x.shape[2]), device = x.device)

    return torch.cat((pad, x), dim=dim)

def loss_function(recon_x, x, mu, logvar):
    MSE = nn.MSELoss()
    mse_loss = MSE(recon_x, x)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return mse_loss + KLD

def train_epoch(model:ConvSeq2Seq, epoch:int, train_loader, target_len:int, device='cpu', print_progress=True) -> Tuple[ConvSeq2Seq, int]:
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    model.to(device)
    model.train()
    train_loss = 0
    for batch_idx, batch in enumerate(train_loader):
        batch = batch[0].to(device)
        X, y = batch[:,:-target_len,:], batch[:,target_len:,:]
        X = pad_tensor(X, target_len)

        optimizer.zero_grad()

        recon_batch, mu, logvar = model(X)
        recon_batch = recon_batch[:,target_len:,:] # Discarding the first target_len predictions

        loss = loss_function(recon_batch, y, mu, logvar)
        loss.backward()
        train_loss += loss.item()

        optimizer.step()

    if print_progress:
        print('====> Epoch: {} Average loss: {:.4f}'.format(
            epoch, train_loss / len(train_loader.dataset)))
    return model, train_loss